{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-47d622d3717d>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-47d622d3717d>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    2to3 -W -n /ddd/cpy/relative_import_example-master\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#2to3 --output-dir=python3-version/mycode -W -n python2-version/mycode \n",
    "\n",
    "2to3 -W -n /ddd/cpy/relative_import_example-master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "import urllib.request, urllib.error, urllib.parse\n",
    "import requests\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from openpyxl import Workbook\n",
    "import imp\n",
    "\n",
    "imp.reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "\n",
    "# Some User Agents\n",
    "hds = [{'User-Agent': 'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6'}, \\\n",
    "       {\n",
    "           'User-Agent': 'Mozilla/5.0 (Windows NT 6.2) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.12 Safari/535.11'}, \\\n",
    "       {'User-Agent': 'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.2; Trident/6.0)'}]\n",
    "\n",
    "\n",
    "def book_spider(book_tag):\n",
    "    page_num = 0;\n",
    "    book_list = []\n",
    "    try_times = 0\n",
    "\n",
    "    while (1):\n",
    "        # url='http://www.douban.com/tag/%E5%B0%8F%E8%AF%B4/book?start=0' # For Test\n",
    "        url = 'http://www.douban.com/tag/' + urllib.parse.quote(book_tag) + '/book?start=' + str(page_num * 15)\n",
    "        time.sleep(np.random.rand() * 5)\n",
    "\n",
    "        # Last Version\n",
    "        try:\n",
    "            req = urllib.request.Request(url, headers=hds[page_num % len(hds)])\n",
    "            source_code = urllib.request.urlopen(req).read()\n",
    "            plain_text = str(source_code)\n",
    "        except (urllib.error.HTTPError, urllib.error.URLError) as e:\n",
    "            print()\n",
    "            e\n",
    "            continue\n",
    "\n",
    "        ##Previous Version, IP is easy to be Forbidden\n",
    "        # source_code = requests.get(url)\n",
    "        # plain_text = source_code.text\n",
    "\n",
    "        soup = BeautifulSoup(plain_text)\n",
    "        list_soup = soup.find('div', {'class': 'mod book-list'})\n",
    "\n",
    "        try_times += 1;\n",
    "        if list_soup == None and try_times < 200:\n",
    "            continue\n",
    "        elif list_soup == None or len(list_soup) <= 1:\n",
    "            break  # Break when no informatoin got after 200 times requesting\n",
    "\n",
    "        for book_info in list_soup.findAll('dd'):\n",
    "            title = book_info.find('a', {'class': 'title'}).string.strip()\n",
    "            desc = book_info.find('div', {'class': 'desc'}).string.strip()\n",
    "            desc_list = desc.split('/')\n",
    "            book_url = book_info.find('a', {'class': 'title'}).get('href')\n",
    "\n",
    "            try:\n",
    "                author_info = '作者/译者： ' + '/'.join(desc_list[0:-3])\n",
    "            except:\n",
    "                author_info = '作者/译者： 暂无'\n",
    "            try:\n",
    "                pub_info = '出版信息： ' + '/'.join(desc_list[-3:])\n",
    "            except:\n",
    "                pub_info = '出版信息： 暂无'\n",
    "            try:\n",
    "                rating = book_info.find('span', {'class': 'rating_nums'}).string.strip()\n",
    "            except:\n",
    "                rating = '0.0'\n",
    "            try:\n",
    "                # people_num = book_info.findAll('span')[2].string.strip()\n",
    "                people_num = get_people_num(book_url)\n",
    "                people_num = people_num.strip('人评价')\n",
    "            except:\n",
    "                people_num = '0'\n",
    "\n",
    "            book_list.append([title, rating, people_num, author_info, pub_info])\n",
    "            try_times = 0  # set 0 when got valid information\n",
    "        page_num += 1\n",
    "        print()\n",
    "        'Downloading Information From Page %d' % page_num\n",
    "    return book_list\n",
    "\n",
    "\n",
    "def get_people_num(url):\n",
    "    # url='http://book.douban.com/subject/6082808/?from=tag_all' # For Test\n",
    "    try:\n",
    "        req = urllib.request.Request(url, headers=hds[np.random.randint(0, len(hds))])\n",
    "        source_code = urllib.request.urlopen(req).read()\n",
    "        plain_text = str(source_code)\n",
    "    except (urllib.error.HTTPError, urllib.error.URLError) as e:\n",
    "        print()\n",
    "        e\n",
    "    soup = BeautifulSoup(plain_text)\n",
    "    people_num = soup.find('div', {'class': 'rating_sum'}).findAll('span')[1].string.strip()\n",
    "    return people_num\n",
    "\n",
    "\n",
    "def do_spider(book_tag_lists):\n",
    "    book_lists = []\n",
    "    for book_tag in book_tag_lists:\n",
    "        book_list = book_spider(book_tag)\n",
    "        book_list = sorted(book_list, key=lambda x: x[1], reverse=True)\n",
    "        book_lists.append(book_list)\n",
    "    return book_lists\n",
    "\n",
    "\n",
    "def print_book_lists_excel(book_lists, book_tag_lists):\n",
    "    wb = Workbook(optimized_write=True)\n",
    "    ws = []\n",
    "    for i in range(len(book_tag_lists)):\n",
    "        ws.append(wb.create_sheet(title=book_tag_lists[i].decode()))  # utf8->unicode\n",
    "    for i in range(len(book_tag_lists)):\n",
    "        ws[i].append(['序号', '书名', '评分', '评价人数', '作者', '出版社'])\n",
    "        count = 1\n",
    "        for bl in book_lists[i]:\n",
    "            ws[i].append([count, bl[0], float(bl[1]), int(bl[2]), bl[3], bl[4]])\n",
    "            count += 1\n",
    "    save_path = 'book_list'\n",
    "    for i in range(len(book_tag_lists)):\n",
    "        save_path += ('-' + book_tag_lists[i].decode())\n",
    "    save_path += '.xlsx'\n",
    "    wb.save(save_path)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # book_tag_lists = ['心理','判断与决策','算法','数据结构','经济','历史']\n",
    "    # book_tag_lists = ['传记','哲学','编程','创业','理财','社会学','佛教']\n",
    "    # book_tag_lists = ['思想','科技','科学','web','股票','爱情','两性']\n",
    "    # book_tag_lists = ['计算机','机器学习','linux','android','数据库','互联网']\n",
    "    # book_tag_lists = ['数学']\n",
    "    # book_tag_lists = ['摄影','设计','音乐','旅行','教育','成长','情感','育儿','健康','养生']\n",
    "    # book_tag_lists = ['商业','理财','管理']\n",
    "    # book_tag_lists = ['名著']\n",
    "    # book_tag_lists = ['科普','经典','生活','心灵','文学']\n",
    "    # book_tag_lists = ['科幻','思维','金融']\n",
    "    book_tag_lists = ['个人管理', '时间管理', '投资', '文化', '宗教']\n",
    "    book_lists = do_spider(book_tag_lists)\n",
    "    print_book_lists_excel(book_lists, book_tag_lists)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
