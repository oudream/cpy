{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----start----\n",
      "p queue join\n",
      "('producer', 2)\n",
      "('producer', 8)\n",
      "('producer', 3)\n",
      "('producer', 9)\n",
      "('producer', 0)\n",
      "('producer', 4)\n",
      "('producer', 1)\n",
      "('producer', 5)\n",
      "('producer', 6)\n",
      "('producer', 7)\n",
      "('consumer', 2)\n",
      "('consumer', 8)\n",
      "('consumer', 3)\n",
      "('consumer', 9)\n",
      "('consumer', 0)\n",
      "('consumer', 4)\n",
      "('consumer', 1)\n",
      "('consumer', 5)\n",
      "('consumer', 6)\n",
      "('consumer', 7)\n",
      "p queue is done & q queue join\n",
      "q queue is done\n",
      "pue empty\n",
      "que empty\n",
      "----end----\n"
     ]
    }
   ],
   "source": [
    "import copy, numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "# compute sigmoid nonlinearity\n",
    "def sigmoid(x):\n",
    "    output = 1/(1+np.exp(-x))\n",
    "    return output\n",
    "\n",
    "# convert output of sigmoid function to its derivative\n",
    "def sigmoid_output_to_derivative(output):\n",
    "    return output*(1-output)\n",
    "\n",
    "\n",
    "# training dataset generation\n",
    "int2binary = {}\n",
    "binary_dim = 8\n",
    "\n",
    "largest_number = pow(2,binary_dim)\n",
    "binary = np.unpackbits(\n",
    "    np.array([list(range(largest_number))],dtype=np.uint8).T,axis=1)\n",
    "for i in range(largest_number):\n",
    "    int2binary[i] = binary[i]\n",
    "\n",
    "\n",
    "# input variables\n",
    "alpha = 0.1\n",
    "input_dim = 2\n",
    "hidden_dim = 16\n",
    "output_dim = 1\n",
    "\n",
    "\n",
    "# initialize neural network weights\n",
    "synapse_0 = 2*np.random.random((input_dim,hidden_dim)) - 1\n",
    "synapse_1 = 2*np.random.random((hidden_dim,output_dim)) - 1\n",
    "synapse_h = 2*np.random.random((hidden_dim,hidden_dim)) - 1\n",
    "\n",
    "synapse_0_update = np.zeros_like(synapse_0)\n",
    "synapse_1_update = np.zeros_like(synapse_1)\n",
    "synapse_h_update = np.zeros_like(synapse_h)\n",
    "\n",
    "# training logic\n",
    "for j in range(10000):\n",
    "    \n",
    "    # generate a simple addition problem (a + b = c)\n",
    "    a_int = np.random.randint(largest_number/2) # int version\n",
    "    a = int2binary[a_int] # binary encoding\n",
    "\n",
    "    b_int = np.random.randint(largest_number/2) # int version\n",
    "    b = int2binary[b_int] # binary encoding\n",
    "\n",
    "    # true answer\n",
    "    c_int = a_int + b_int\n",
    "    c = int2binary[c_int]\n",
    "    \n",
    "    # where we'll store our best guess (binary encoded)\n",
    "    d = np.zeros_like(c)\n",
    "\n",
    "    overallError = 0\n",
    "    \n",
    "    layer_2_deltas = list()\n",
    "    layer_1_values = list()\n",
    "    layer_1_values.append(np.zeros(hidden_dim))\n",
    "    \n",
    "    # moving along the positions in the binary encoding\n",
    "    for position in range(binary_dim):\n",
    "        \n",
    "        # generate input and output\n",
    "        X = np.array([[a[binary_dim - position - 1],b[binary_dim - position - 1]]])\n",
    "        y = np.array([[c[binary_dim - position - 1]]]).T\n",
    "\n",
    "        # hidden layer (input ~+ prev_hidden)\n",
    "        layer_1 = sigmoid(np.dot(X,synapse_0) + np.dot(layer_1_values[-1],synapse_h))\n",
    "\n",
    "        # output layer (new binary representation)\n",
    "        layer_2 = sigmoid(np.dot(layer_1,synapse_1))\n",
    "\n",
    "        # did we miss?... if so, by how much?\n",
    "        layer_2_error = y - layer_2\n",
    "        layer_2_deltas.append((layer_2_error)*sigmoid_output_to_derivative(layer_2))\n",
    "        overallError += np.abs(layer_2_error[0])\n",
    "    \n",
    "        # decode estimate so we can print it out\n",
    "        d[binary_dim - position - 1] = np.round(layer_2[0][0])\n",
    "        \n",
    "        # store hidden layer so we can use it in the next timestep\n",
    "        layer_1_values.append(copy.deepcopy(layer_1))\n",
    "    \n",
    "    future_layer_1_delta = np.zeros(hidden_dim)\n",
    "    \n",
    "    for position in range(binary_dim):\n",
    "        \n",
    "        X = np.array([[a[position],b[position]]])\n",
    "        layer_1 = layer_1_values[-position-1]\n",
    "        prev_layer_1 = layer_1_values[-position-2]\n",
    "        \n",
    "        # error at output layer\n",
    "        layer_2_delta = layer_2_deltas[-position-1]\n",
    "        # error at hidden layer\n",
    "        layer_1_delta = (future_layer_1_delta.dot(synapse_h.T) + layer_2_delta.dot(synapse_1.T)) * sigmoid_output_to_derivative(layer_1)\n",
    "\n",
    "        # let's update all our weights so we can try again\n",
    "        synapse_1_update += np.atleast_2d(layer_1).T.dot(layer_2_delta)\n",
    "        synapse_h_update += np.atleast_2d(prev_layer_1).T.dot(layer_1_delta)\n",
    "        synapse_0_update += X.T.dot(layer_1_delta)\n",
    "        \n",
    "        future_layer_1_delta = layer_1_delta\n",
    "    \n",
    "\n",
    "    synapse_0 += synapse_0_update * alpha\n",
    "    synapse_1 += synapse_1_update * alpha\n",
    "    synapse_h += synapse_h_update * alpha    \n",
    "\n",
    "    synapse_0_update *= 0\n",
    "    synapse_1_update *= 0\n",
    "    synapse_h_update *= 0\n",
    "    \n",
    "    # print out progress\n",
    "    if(j % 1000 == 0):\n",
    "        print(\"Error:\" + str(overallError))\n",
    "        print(\"Pred:\" + str(d))\n",
    "        print(\"True:\" + str(c))\n",
    "        out = 0\n",
    "        for index,x in enumerate(reversed(d)):\n",
    "            out += x*pow(2,index)\n",
    "        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out))\n",
    "        print(\"------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "'''RNN参数设置，计算8位二进制加法，因此序列长度为8，每次向place_holder中喂入batch_size大小的数据，包括两个相加的二进制序列和一个结果二进制序列,hidden_size表示RNN的隐层节点维数'''\n",
    "sequence_length = 8\n",
    "batch_size = 100\n",
    "input_size = 2\n",
    "hidden_size = 8\n",
    "output_size = 1\n",
    "\n",
    "'''推断过程，即是RNN模型的构建过程'''\n",
    "def inference(x_input):\n",
    "'''将输入转换到tensorflow中RNN需要的格式'''\n",
    "    x = tf.transpose(x_input, [1, 0, 2])\n",
    "    x = tf.reshape(x, [-1, input_size])\n",
    "    x = tf.split(0, sequence_length, x)\n",
    "    '''构建rnn单元和rnn网络，简单两句搞定，关键在与返回值上，输出看了下，states只有两个值，应该一个是初始状态，一个是末状态，而其他的中间状态其实涵盖在outputs里了，需要直接去取就好，不知道LSTM的输出是怎样，states会不会是8个，得去试一下'''\n",
    "    rnn_cell = tf.nn.rnn_cell.BasicRNNCell(8)\n",
    "    outputs, states = tf.nn.rnn(rnn_cell, x, dtype=tf.float16)\n",
    "\n",
    "'''定义输出层权值和偏置，用variable_scope和get_variable是为了变量的不重复，否则测试和训练会构建重复的variable出来，Tensorboard里面网络的graph会变的比较乱'''\n",
    "    with tf.variable_scope('hidden_to_output_layer'):\n",
    "        w2out = tf.get_variable('w2out', shape=[8, 1], dtype=tf.float16, initializer=tf.truncated_normal_initializer(0.0, 1.0))\n",
    "        bias2out = tf.get_variable('bias2oput', shape=[1], dtype=tf.float16, initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "    outputs = tf.reshape(outputs, [-1, hidden_size])\n",
    "    result = tf.nn.sigmoid(tf.matmul(outputs, w2out)+bias2out) \n",
    "    result = tf.reshape(result, [data_length, -1])\n",
    "    result = tf.transpose(result, [1, 0])\n",
    "    return result\n",
    "\n",
    "def Loss(logits, y_true):\n",
    "    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits, y_true))\n",
    "    #softmax_cross_entropy_with_logits will find the largest point in logits and y_ture,then compare them.\n",
    "\n",
    "def generateBinaryDict():\n",
    "    largest_number = pow(2, 8)\n",
    "    int2binary={}\n",
    "    binary = np.unpackbits(\n",
    "        np.array([range(largest_number)], dtype=np.uint8).T, axis=1)\n",
    "    for i in range(largest_number):\n",
    "        int2binary[i] = binary[i]\n",
    "    return int2binary\n",
    "\n",
    "\n",
    "input_holder = tf.placeholder(tf.float16, [None, 8, 2])\n",
    "label_holder = tf.placeholder(tf.float16, [None, 8])\n",
    "y_pred = inference(input_holder)\n",
    "loss = tf.reduce_mean(tf.square(y_pred-label_holder))\n",
    "#loss = Loss(y_pred, label_holder)\n",
    "#op = tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n",
    "op = tf.train.AdagradOptimizer(1.0).minimize(loss)\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    binary_dict = generateBinaryDict()\n",
    "    epochs = 10000\n",
    "    for e in range(epochs):\n",
    "        batch_in_data = []\n",
    "        batch_out_data = []\n",
    "        reshape_bi_list = []\n",
    "        '''生成minibatch data，其实没必要，但是为了规范'''\n",
    "        for i in range(batch_size):\n",
    "            x_int1 = np.random.randint(0, 128)\n",
    "            x_int2 = np.random.randint(0, 128)\n",
    "            y_true_int = x_int1 + x_int2\n",
    "\n",
    "            x_bi1 = binary_dict[x_int1]\n",
    "            x_bi2 = binary_dict[x_int2]\n",
    "            y_true_bi = binary_dict[y_true_int]\n",
    "            '''注意输入和输出都需要反向，因为建的RNN是从前往后传信息的，而二进制加法的进位规则是从后往前计算并进位的'''\n",
    "            for j in range(8):\n",
    "                reshape_bi_list.append(x_bi1[7-j])\n",
    "                reshape_bi_list.append(x_bi2[7-j])\n",
    "            batch_out_data.append(y_true_bi[::-1])\n",
    "\n",
    "        batch_in_data = np.reshape(reshape_bi_list, [-1, 8, 2])\n",
    "        batch_out_data = np.reshape(batch_out_data, [-1, 8])\n",
    "\n",
    "        sess.run(op, feed_dict={input_holder: batch_in_data, label_holder: batch_out_data})\n",
    "        loss_value = sess.run(loss, feed_dict={input_holder: batch_in_data, label_holder: batch_out_data})\n",
    "        print(('iteration at: % d, mean loss is: %f' % (e, loss_value)))\n",
    "\n",
    "        '''Test part'''\n",
    "        x_int1_test = np.random.randint(0, 128)\n",
    "        x_int2_test = np.random.randint(0, 128)\n",
    "        y_true_int_test = x_int1_test + x_int2_test\n",
    "\n",
    "        x_bi1_test = binary_dict[x_int1_test]\n",
    "        x_bi2_test = binary_dict[x_int2_test]\n",
    "        y_bi_true = binary_dict[y_true_int_test]\n",
    "        batch_in_test = []\n",
    "        reshape_bi_list_test = []\n",
    "        # batch_in_test.append(np.concatenate((x_bi1_test[::-1], x_bi2_test[::-1]), axis=0))\n",
    "        for k in range(8):\n",
    "            reshape_bi_list_test.append(x_bi1_test[7 - k])\n",
    "            reshape_bi_list_test.append(x_bi2_test[7 - k])\n",
    "        batch_in_test = np.reshape(reshape_bi_list_test, [1, 8, 2])\n",
    "\n",
    "        test_result = sess.run(y_pred, feed_dict={input_holder: batch_in_test})\n",
    "        print(( 'x_bi1_test: ', x_bi1_test ))\n",
    "        print(( 'x_bi2_test: ', x_bi2_test ))\n",
    "        print(( 'true_result:', y_bi_true ))\n",
    "        print(( 'predict:    ', np.int8(np.round(test_result)[0][::-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    binary_dict = generateBinaryDict()\n",
    "    epochs = 10000\n",
    "    for e in range(epochs):\n",
    "        batch_in_data = []\n",
    "        batch_out_data = []\n",
    "        reshape_bi_list = []\n",
    "        '''生成minibatch data，其实没必要，但是为了规范'''\n",
    "        for i in range(batch_size):\n",
    "            x_int1 = np.random.randint(0, 128)\n",
    "            x_int2 = np.random.randint(0, 128)\n",
    "            y_true_int = x_int1 + x_int2\n",
    "\n",
    "            x_bi1 = binary_dict[x_int1]\n",
    "            x_bi2 = binary_dict[x_int2]\n",
    "            y_true_bi = binary_dict[y_true_int]\n",
    "            '''注意输入和输出都需要反向，因为建的RNN是从前往后传信息的，而二进制加法的进位规则是从后往前计算并进位的'''\n",
    "            for j in range(8):\n",
    "                reshape_bi_list.append(x_bi1[7-j])\n",
    "                reshape_bi_list.append(x_bi2[7-j])\n",
    "            batch_out_data.append(y_true_bi[::-1])\n",
    "\n",
    "        batch_in_data = np.reshape(reshape_bi_list, [-1, 8, 2])\n",
    "        batch_out_data = np.reshape(batch_out_data, [-1, 8])\n",
    "\n",
    "        sess.run(op, feed_dict={input_holder: batch_in_data, label_holder: batch_out_data})\n",
    "        loss_value = sess.run(loss, feed_dict={input_holder: batch_in_data, label_holder: batch_out_data})\n",
    "        print(('iteration at: % d, mean loss is: %f' % (e, loss_value)))\n",
    "\n",
    "        '''Test part'''\n",
    "        x_int1_test = np.random.randint(0, 128)\n",
    "        x_int2_test = np.random.randint(0, 128)\n",
    "        y_true_int_test = x_int1_test + x_int2_test\n",
    "\n",
    "        x_bi1_test = binary_dict[x_int1_test]\n",
    "        x_bi2_test = binary_dict[x_int2_test]\n",
    "        y_bi_true = binary_dict[y_true_int_test]\n",
    "        batch_in_test = []\n",
    "        reshape_bi_list_test = []\n",
    "        # batch_in_test.append(np.concatenate((x_bi1_test[::-1], x_bi2_test[::-1]), axis=0))\n",
    "        for k in range(8):\n",
    "            reshape_bi_list_test.append(x_bi1_test[7 - k])\n",
    "            reshape_bi_list_test.append(x_bi2_test[7 - k])\n",
    "        batch_in_test = np.reshape(reshape_bi_list_test, [1, 8, 2])\n",
    "\n",
    "        test_result = sess.run(y_pred, feed_dict={input_holder: batch_in_test})\n",
    "        print(( 'x_bi1_test: ', x_bi1_test ))\n",
    "        print(( 'x_bi2_test: ', x_bi2_test ))\n",
    "        print(( 'true_result:', y_bi_true ))\n",
    "        print(( 'predict:    ', np.int8(np.round(test_result)[0][::-1])))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "1_notmnist.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
