{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN構造二進制加法器的tensorflow實現\n",
    "\n",
    "RNN（循环神经网络）对于序列数据的建模有得天独厚的优势，相比一般的前馈神经网络，它对于历史输入数据具有一定的记忆性，能通过隐变量记录历史信息。本文利用RNN来学习二进制加法的进位规则。\n",
    "\n",
    "异国友人写了一个Python+Numpy的RNN二进制加法器，简单明了，还包括作者对RNN的理解和介绍，非常棒的博客。地址在这里。本文是针对它的tensorflow实现，一方面学习RNN，一方面学习tensorflow的使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.contrib.rnn' has no attribute 'core_rnn_cell'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4982d9c9caae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0minput_holder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mlabel_holder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_holder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlabel_holder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m# loss = Loss(y_pred, label_holder)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-4982d9c9caae>\u001b[0m in \u001b[0;36minference\u001b[0;34m(x_input)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m#     '''构建rnn单元和rnn网络，简单两句搞定，关键在与返回值上，输出看了下，states只有两个值，应该一个是初始状态，一个是末状态，而其他的中间状态其实涵盖在outputs里了，需要直接去取就好，不知道LSTM的输出是怎样，states会不会是8个，得去试一下'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mrnn_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore_rnn_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_cell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# '''定义输出层权值和偏置，用variable_scope和get_variable是为了变量的不重复，否则测试和训练会构建重复的variable出来，Tensorboard里面网络的graph会变的比较乱'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.contrib.rnn' has no attribute 'core_rnn_cell'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# '''RNN参数设置，计算8位二进制加法，因此序列长度为8，每次向place_holder中喂入batch_size大小的数据，包括两个相加的二进制序列和一个结果二进制序列,hidden_size表示RNN的隐层节点维数'''\n",
    "sequence_length = 8\n",
    "batch_size = 100\n",
    "input_size = 2\n",
    "hidden_size = 8\n",
    "output_size = 1\n",
    "\n",
    "# '''推断过程，即是RNN模型的构建过程'''\n",
    "def inference(x_input):\n",
    "    # '''将输入转换到tensorflow中RNN需要的格式'''\n",
    "    x = tf.transpose(x_input, [1, 0, 2])\n",
    "    x = tf.reshape(x, [-1, input_size])\n",
    "    x = tf.split(0, sequence_length, x)\n",
    "    #     '''构建rnn单元和rnn网络，简单两句搞定，关键在与返回值上，输出看了下，states只有两个值，应该一个是初始状态，一个是末状态，而其他的中间状态其实涵盖在outputs里了，需要直接去取就好，不知道LSTM的输出是怎样，states会不会是8个，得去试一下'''\n",
    "    rnn_cell = tf.contrib.rnn.core_rnn_cell(8)\n",
    "    outputs, states = tf.nn.rnn(rnn_cell, x, dtype=tf.float16)\n",
    "    # '''定义输出层权值和偏置，用variable_scope和get_variable是为了变量的不重复，否则测试和训练会构建重复的variable出来，Tensorboard里面网络的graph会变的比较乱'''\n",
    "    with tf.variable_scope('hidden_to_output_layer'):\n",
    "        w2out = tf.get_variable('w2out', shape=[8, 1], dtype=tf.float16, initializer=tf.truncated_normal_initializer(0.0, 1.0))\n",
    "        bias2out = tf.get_variable('bias2oput', shape=[1], dtype=tf.float16, initializer=tf.constant_initializer(0.0))\n",
    "    outputs = tf.reshape(outputs, [-1, hidden_size])\n",
    "    result = tf.nn.sigmoid(tf.matmul(outputs, w2out)+bias2out) \n",
    "    result = tf.reshape(result, [data_length, -1])\n",
    "    result = tf.transpose(result, [1, 0])\n",
    "    return result\n",
    "\n",
    "def Loss(logits, y_true):\n",
    "    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits, y_true))\n",
    "    #softmax_cross_entropy_with_logits will find the largest point in logits and y_ture,then compare them.\n",
    "\n",
    "def generateBinaryDict():\n",
    "    largest_number = pow(2, 8)\n",
    "    int2binary={}\n",
    "    binary = np.unpackbits(\n",
    "        np.array([list(range(largest_number))], dtype=np.uint8).T, axis=1)\n",
    "    for i in range(largest_number):\n",
    "        int2binary[i] = binary[i]\n",
    "    return int2binary\n",
    "\n",
    "\n",
    "input_holder = tf.placeholder(tf.int32, [None, 8, 2])\n",
    "label_holder = tf.placeholder(tf.int32, [None, 8])\n",
    "y_pred = inference(input_holder)\n",
    "loss = tf.reduce_mean(tf.square(y_pred-label_holder))\n",
    "# loss = Loss(y_pred, label_holder)\n",
    "# op = tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n",
    "op = tf.train.AdagradOptimizer(1.0).minimize(loss)\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    binary_dict = generateBinaryDict()\n",
    "    epochs = 10000\n",
    "    for e in range(epochs):\n",
    "        batch_in_data = []\n",
    "        batch_out_data = []\n",
    "        reshape_bi_list = []\n",
    "#         '''生成minibatch data，其实没必要，但是为了规范'''\n",
    "        for i in range(batch_size):\n",
    "            x_int1 = np.random.randint(0, 128)\n",
    "            x_int2 = np.random.randint(0, 128)\n",
    "            y_true_int = x_int1 + x_int2\n",
    "\n",
    "            x_bi1 = binary_dict[x_int1]\n",
    "            x_bi2 = binary_dict[x_int2]\n",
    "            y_true_bi = binary_dict[y_true_int]\n",
    "#             '''注意输入和输出都需要反向，因为建的RNN是从前往后传信息的，而二进制加法的进位规则是从后往前计算并进位的'''\n",
    "            for j in range(8):\n",
    "                reshape_bi_list.append(x_bi1[7-j])\n",
    "                reshape_bi_list.append(x_bi2[7-j])\n",
    "            batch_out_data.append(y_true_bi[::-1])\n",
    "\n",
    "        batch_in_data = np.reshape(reshape_bi_list, [-1, 8, 2])\n",
    "        batch_out_data = np.reshape(batch_out_data, [-1, 8])\n",
    "\n",
    "        sess.run(op, feed_dict={input_holder: batch_in_data, label_holder: batch_out_data})\n",
    "        loss_value = sess.run(loss, feed_dict={input_holder: batch_in_data, label_holder: batch_out_data})\n",
    "        print(('iteration at: % d, mean loss is: %f' % (e, loss_value)))\n",
    "\n",
    "#         '''Test part'''\n",
    "        x_int1_test = np.random.randint(0, 128)\n",
    "        x_int2_test = np.random.randint(0, 128)\n",
    "        y_true_int_test = x_int1_test + x_int2_test\n",
    "\n",
    "        x_bi1_test = binary_dict[x_int1_test]\n",
    "        x_bi2_test = binary_dict[x_int2_test]\n",
    "        y_bi_true = binary_dict[y_true_int_test]\n",
    "        batch_in_test = []\n",
    "        reshape_bi_list_test = []\n",
    "        # batch_in_test.append(np.concatenate((x_bi1_test[::-1], x_bi2_test[::-1]), axis=0))\n",
    "        for k in range(8):\n",
    "            reshape_bi_list_test.append(x_bi1_test[7 - k])\n",
    "            reshape_bi_list_test.append(x_bi2_test[7 - k])\n",
    "        batch_in_test = np.reshape(reshape_bi_list_test, [1, 8, 2])\n",
    "\n",
    "        test_result = sess.run(y_pred, feed_dict={input_holder: batch_in_test})\n",
    "        print(('x_bi1_test: ', x_bi1_test))\n",
    "        print(('x_bi2_test: ', x_bi2_test))\n",
    "        print(('true_result:', y_bi_true))\n",
    "        print(('predict:    ', np.int8(np.round(test_result)[0][::-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
