{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1]\n",
      "[1 1]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "a = tf.Variable([1, 1])\n",
    "b = tf.Variable([1, 1])\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(sess.run(a))\n",
    "    print(sess.run(b))\n",
    "    writer = tf.summary.FileWriter(\"/tmp/test/3\")\n",
    "    writer.add_graph(sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /ddd/cpy/cpy/MNIST/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting /ddd/cpy/cpy/MNIST/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting /ddd/cpy/cpy/MNIST/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /ddd/cpy/cpy/MNIST/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "step 0, training accuracy 0.07\n",
      "step 50, training accuracy 0.1\n",
      "step 100, training accuracy 0.09\n",
      "step 150, training accuracy 0.12\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "LOGDIR = \"/ddd/cpy/cpy/MNIST/MNIST_data/\"\n",
    "mnist = tf.contrib.learn.datasets.mnist.read_data_sets(train_dir=LOGDIR, one_hot=True)\n",
    "\n",
    "# Define a simple convolutional layer\n",
    "def conv_layer(input, channels_in, channels_out):\n",
    "    w = tf.Variable(tf.zeros([5, 5, channels_in, channels_out]))\n",
    "    b = tf.Variable(tf.zeros([channels_out]))\n",
    "    conv = tf.nn.conv2d(input, w, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "    act = tf.nn.relu(conv + b)\n",
    "    return act\n",
    "\n",
    "\n",
    "# Define a fully connected layer\n",
    "def fc_layer(input, channels_in, channels_out):\n",
    "    w = tf.Variable(tf.zeros([channels_in, channels_out]))\n",
    "    b = tf.Variable(tf.zeros([channels_out]))\n",
    "    act = tf.nn.relu(tf.matmul(input, w) + b)\n",
    "    return act\n",
    "\n",
    "\n",
    "# Setup placeholders, and reshape the data\n",
    "# mnist images are 28 x 28 = 784\n",
    "# output should be 10 to correspond to 10 digits\n",
    "# need to turn mnist vectors into an image\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "\n",
    "# create the network\n",
    "conv1 = conv_layer(x_image, 1, 32)\n",
    "pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "\n",
    "conv2 = conv_layer(pool1, 32, 64)\n",
    "pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "flattened = tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
    "\n",
    "fc1 = fc_layer(flattened, 7 * 7 * 64, 1024)\n",
    "logits = fc_layer(fc1, 1024, 10)\n",
    "\n",
    "# Compute cross entropy as the loss function\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "\n",
    "# Use an AdamOptimizer to train the network\n",
    "train_step = tf.train.AdadeltaOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "# Compute the accuracy\n",
    "correct_predition = tf.equal(tf.arg_max(logits, 1), tf.arg_max(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predition, tf.float32))\n",
    "\n",
    "\n",
    "# Initialize the variables\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Train for 2000 steps\n",
    "for i in range(200):\n",
    "    batch = mnist.train.next_batch(100)\n",
    "\n",
    "    # Occasionally report accuracy\n",
    "    if i % 50 == 0:\n",
    "        [train_accuracy] = sess.run([accuracy], feed_dict={x: batch[0], y:batch[1]})\n",
    "        print(\"step %d, training accuracy %g\" % (i, train_accuracy))\n",
    "\n",
    "    # Run the training step\n",
    "    sess.run(train_step, feed_dict={x: batch[0], y: batch[1]})\n",
    "\n",
    "writer = tf.summary.FileWriter(\"/tmp/mnist_demo/1\")\n",
    "writer.add_graph(sess.graph)\n",
    "\n",
    "print(\"finish\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /ddd/cpy/cpy/MNIST/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting /ddd/cpy/cpy/MNIST/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting /ddd/cpy/cpy/MNIST/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /ddd/cpy/cpy/MNIST/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "step 0, training accuracy 0.09\n",
      "step 5, training accuracy 0.15\n",
      "step 10, training accuracy 0.3\n",
      "step 15, training accuracy 0.36\n",
      "step 20, training accuracy 0.46\n",
      "step 25, training accuracy 0.54\n",
      "step 30, training accuracy 0.67\n",
      "step 35, training accuracy 0.66\n",
      "step 40, training accuracy 0.66\n",
      "step 45, training accuracy 0.73\n",
      "step 50, training accuracy 0.74\n",
      "step 55, training accuracy 0.79\n",
      "step 60, training accuracy 0.79\n",
      "step 65, training accuracy 0.72\n",
      "step 70, training accuracy 0.87\n",
      "step 75, training accuracy 0.83\n",
      "step 80, training accuracy 0.87\n",
      "step 85, training accuracy 0.84\n",
      "step 90, training accuracy 0.8\n",
      "step 95, training accuracy 0.81\n",
      "step 100, training accuracy 0.88\n",
      "step 105, training accuracy 0.85\n",
      "step 110, training accuracy 0.88\n",
      "step 115, training accuracy 0.91\n",
      "step 120, training accuracy 0.85\n",
      "step 125, training accuracy 0.85\n",
      "step 130, training accuracy 0.92\n",
      "step 135, training accuracy 0.87\n",
      "step 140, training accuracy 0.92\n",
      "step 145, training accuracy 0.91\n",
      "step 150, training accuracy 0.93\n",
      "step 155, training accuracy 0.91\n",
      "step 160, training accuracy 0.9\n",
      "step 165, training accuracy 0.94\n",
      "step 170, training accuracy 0.92\n",
      "step 175, training accuracy 0.89\n",
      "step 180, training accuracy 0.89\n",
      "step 185, training accuracy 0.89\n",
      "step 190, training accuracy 0.95\n",
      "step 195, training accuracy 0.91\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# example from tensorflow dev summit 2017 dandelion mane\n",
    "\n",
    "LOGDIR = \"/ddd/cpy/cpy/MNIST/MNIST_data/\"\n",
    "mnist = tf.contrib.learn.datasets.mnist.read_data_sets(train_dir=LOGDIR, one_hot=True)\n",
    "\n",
    "\n",
    "# Define a simple convolutional layer\n",
    "def conv_layer(input, channels_in, channels_out, name=\"conv\"):\n",
    "    with tf.name_scope(name):\n",
    "        w = tf.Variable(tf.truncated_normal([5, 5, channels_in, channels_out], stddev=0.1), name=\"W\")\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[channels_out]), name=\"B\")\n",
    "        conv = tf.nn.conv2d(input, w, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "        act = tf.nn.relu(conv + b)\n",
    "        tf.summary.histogram(\"weights\", w)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"activations\", act)\n",
    "        return tf.nn.max_pool(act, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "\n",
    "\n",
    "# Define a fully connected layer\n",
    "def fc_layer(input, channels_in, channels_out, name=\"fc\"):\n",
    "    with tf.name_scope(name):\n",
    "        w = tf.Variable(tf.truncated_normal([channels_in, channels_out], stddev=0.1, name=\"W\"))\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[channels_out]), name=\"B\")\n",
    "        act = tf.matmul(input, w) + b\n",
    "        tf.summary.histogram(\"weights\", w)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"activations\", act)\n",
    "        return act\n",
    "\n",
    "\n",
    "# Setup placeholders, and reshape the data\n",
    "# mnist images are 28 x 28 = 784\n",
    "# output should be 10 to correspond to 10 digits\n",
    "# need to turn mnist vectors into an image\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784], name=\"x\")\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10], name=\"labels\")\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "tf.summary.image(\"input\", x_image, 3)\n",
    "\n",
    "# create the network\n",
    "conv1 = conv_layer(x_image, 1, 32, \"conv1\")\n",
    "conv2 = conv_layer(conv1, 32, 64, \"conv2\")\n",
    "\n",
    "flattened = tf.reshape(conv2, [-1, 7 * 7 * 64])\n",
    "fc1 = fc_layer(flattened, 7 * 7 * 64, 1024, \"fc1\")\n",
    "logits = fc_layer(fc1, 1024, 10, \"fc2\")\n",
    "\n",
    "# Compute cross entropy as the loss function\n",
    "with tf.name_scope(\"xent\"):\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y), name=\"xent\")\n",
    "    tf.summary.scalar(\"xent\", cross_entropy)\n",
    "\n",
    "# Use an AdamOptimizer to train the network\n",
    "with tf.name_scope(\"train\"):\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "# Compute the accuracy\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "# Initialize the variables\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "merged_summary = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(\"/tmp/mnist_demo/3\")\n",
    "writer.add_graph(sess.graph)\n",
    "\n",
    "# Train for 2001 steps\n",
    "for i in range(200):\n",
    "    batch = mnist.train.next_batch(100)\n",
    "\n",
    "    # Occasionally report accuracy\n",
    "    if i % 5 == 0:\n",
    "        [train_accuracy, s] = sess.run([accuracy, merged_summary], feed_dict={x: batch[0], y: batch[1]})\n",
    "        writer.add_summary(s, i)\n",
    "        print(\"step %d, training accuracy %g\" % (i, train_accuracy))\n",
    "\n",
    "    # Run the training step\n",
    "    sess.run(train_step, feed_dict={x: batch[0], y: batch[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
